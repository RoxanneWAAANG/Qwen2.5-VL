# Qwen2.5-VL -- Linux A100 CUDA 12.7

(qwenvl-new) jack@user:~/Projects/yixin-llm/Qwen2.5-VL/qwen-vl-finetune$ pip show torch flash-attn deepspeed
Name: torch
Version: 2.4.0+cu121
Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Home-page: https://pytorch.org/
Author: PyTorch Team
Author-email: packages@pytorch.org
License: BSD-3
Location: /home/jack/anaconda3/envs/qwenvl-new/lib/python3.11/site-packages
Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions
Required-by: accelerate, bitsandbytes, deepspeed, flash-attn, peft, torchaudio, torchvision
---
Name: flash-attn
Version: 2.6.3
Summary: Flash Attention: Fast and Memory-Efficient Exact Attention
Home-page: https://github.com/Dao-AILab/flash-attention
Author: Tri Dao
Author-email: tri@tridao.me
License: 
Location: /home/jack/anaconda3/envs/qwenvl-new/lib/python3.11/site-packages
Requires: einops, torch
Required-by: 
---
Name: deepspeed
Version: 0.15.4
Summary: DeepSpeed library
Home-page: http://deepspeed.ai
Author: DeepSpeed Team
Author-email: deepspeed-info@microsoft.com
License: Apache Software License 2.0
Location: /home/jack/anaconda3/envs/qwenvl-new/lib/python3.11/site-packages
Requires: hjson, msgpack, ninja, numpy, nvidia-ml-py, packaging, psutil, py-cpuinfo, pydantic, torch, tqdm
Required-by: 

## æ­¥éª¤ 1: ç¯å¢ƒæ¸…ç†

```bash
# å®Œå…¨åˆ é™¤ç°æœ‰condaç¯å¢ƒ
conda deactivate
conda env remove -n qwenvl-new

# æ¸…ç†æ‰€æœ‰ç¼“å­˜
pip cache purge
rm -rf ~/.cache/huggingface/
rm -rf ~/.cache/torch/
```

## æ­¥éª¤ 2: åˆ›å»ºæ–°ç¯å¢ƒ

```bash
conda create -n qwenvl-new python=3.11 -y
conda activate qwenvl-new

conda update conda -y
pip install --upgrade pip setuptools wheel
```

## æ­¥éª¤ 3: é…ç½®CUDAç¯å¢ƒ

```bash
# æ£€æŸ¥å¯ç”¨çš„CUDAç‰ˆæœ¬
ls /usr/local/cuda*/bin/nvcc

# è®¾ç½®CUDA_HOME (æ ¹æ®ä½ çš„ç³»ç»Ÿé€‰æ‹©12.4æˆ–12.6)
export CUDA_HOME=/usr/local/cuda-12.4
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# æ°¸ä¹…ä¿å­˜åˆ°bashrc
echo 'export CUDA_HOME=/usr/local/cuda-12.4' >> ~/.bashrc
echo 'export PATH=$CUDA_HOME/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# éªŒè¯CUDAè®¾ç½®
nvcc --version
nvidia-smi
```

## æ­¥éª¤ 4: å®‰è£…PyTorch

```bash
# å®‰è£…PyTorch 2.4.0 (æ”¯æŒCUDA 12.1+)
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121

# éªŒè¯PyTorchå®‰è£…
python -c "
import torch
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')
print(f'GPUæ•°é‡: {torch.cuda.device_count()}')
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
"
```

## æ­¥éª¤ 5: å®‰è£…å…¼å®¹çš„æ ¸å¿ƒåº“ (å…³é”®æ­¥éª¤)

```bash
# å®‰è£…ç»è¿‡æµ‹è¯•çš„å…¼å®¹ç‰ˆæœ¬ç»„åˆ
pip install accelerate==0.34.2
pip install transformers==4.45.2
pip install tokenizers==0.20.0
pip install datasets==3.0.1
pip install peft==0.12.0

# éªŒè¯æ ¸å¿ƒåº“å®‰è£…
python -c "
import transformers
import accelerate
print(f'Transformers: {transformers.__version__}')
print(f'Accelerate: {accelerate.__version__}')

try:
    from transformers import Trainer
    print('âœ… Trainer å¯¼å…¥æˆåŠŸ')
except ImportError as e:
    print(f'âŒ Trainer å¯¼å…¥å¤±è´¥: {e}')
"
```

## æ­¥éª¤ 6: å®‰è£…Flash Attention

```bash
# å®‰è£…ç¼–è¯‘ä¾èµ–
pip install packaging ninja wheel

# æ–¹æ³•1: å°è¯•å®‰è£…Flash Attention (å¯èƒ½éœ€è¦ç¼–è¯‘æ—¶é—´)
pip install flash-attn==2.5.9 --no-build-isolation

# å¦‚æœç¼–è¯‘å¤±è´¥ï¼Œå°è¯•æ–¹æ³•2: ä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬
# pip install flash-attn --find-links https://github.com/Dao-AILab/flash-attention/releases

# éªŒè¯Flash Attention (å¯é€‰ï¼Œä¸æ˜¯å¿…éœ€çš„)
python -c "
try:
    import flash_attn
    print(f'âœ… Flash Attentionç‰ˆæœ¬: {flash_attn.__version__}')
except ImportError:
    print('âŒ Flash Attentionæœªå®‰è£…ï¼Œå°†ä½¿ç”¨å…¶ä»–attentionå®ç°')
"
```

## æ­¥éª¤ 7: å®‰è£…å…¶ä»–ä¾èµ–

```bash
# å®‰è£…æ·±åº¦å­¦ä¹ ç›¸å…³åº“
pip install deepspeed==0.14.4
pip install bitsandbytes==0.43.3

# å®‰è£…å›¾åƒå¤„ç†åº“
pip install Pillow==10.4.0
pip install opencv-python==4.10.0.84

# å®‰è£…å·¥å…·åº“
pip install tqdm
pip install tensorboard
pip install wandb  # å®éªŒè¿½è¸ª (å¯é€‰)
pip install scikit-learn
pip install matplotlib seaborn

# å®‰è£…Qwenç‰¹å®šä¾èµ–
pip install tiktoken
pip install transformers_stream_generator
```

## æ­¥éª¤ 8: éªŒè¯Qwen2.5-VLæ”¯æŒ

```bash
# åˆ›å»ºå®Œæ•´éªŒè¯è„šæœ¬
cat > verify_qwen_setup.py << 'EOF'
#!/usr/bin/env python3
import sys

def check_qwen_support():
    print("=== Qwen2.5-VL ç¯å¢ƒéªŒè¯ ===")
    
    # åŸºç¡€ç¯å¢ƒæ£€æŸ¥
    import torch
    import transformers
    import accelerate
    
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
    print(f"Accelerateç‰ˆæœ¬: {accelerate.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    # æ£€æŸ¥Trainerå¯¼å…¥
    try:
        from transformers import Trainer
        print("âœ… Trainer å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ Trainer å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥Qwen2.5-VLæ¨¡å‹æ”¯æŒ
    try:
        from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5VLForConditionalGeneration
        print("âœ… Qwen2.5-VL æ¨¡å‹æ”¯æŒæ­£å¸¸")
    except ImportError as e:
        print(f"âŒ Qwen2.5-VL æ¨¡å‹æ”¯æŒå¼‚å¸¸: {e}")
        
        # å°è¯•é€šç”¨å¯¼å…¥æ–¹å¼
        try:
            from transformers import AutoModelForCausalLM
            print("âœ… å¯ä»¥ä½¿ç”¨AutoModelä½œä¸ºæ›¿ä»£")
        except ImportError:
            print("âŒ æ— æ³•å¯¼å…¥ä»»ä½•æ¨¡å‹ç±»")
            return False
    
    # æ£€æŸ¥Flash Attention
    try:
        import flash_attn
        print(f"âœ… Flash Attentionå¯ç”¨: {flash_attn.__version__}")
    except ImportError:
        print("âš ï¸  Flash Attentionæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ ‡å‡†attention")
    
    # æµ‹è¯•CUDAæ“ä½œ
    if torch.cuda.is_available():
        try:
            x = torch.randn(10, 10).cuda()
            y = torch.randn(10, 10).cuda()
            z = torch.mm(x, y)
            print("âœ… CUDAå¼ é‡æ“ä½œæµ‹è¯•æˆåŠŸ")
        except Exception as e:
            print(f"âŒ CUDAå¼ é‡æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
            return False
    
    print("\nğŸ‰ ç¯å¢ƒé…ç½®éªŒè¯å®Œæˆ!")
    return True

if __name__ == "__main__":
    success = check_qwen_support()
    sys.exit(0 if success else 1)
EOF

# è¿è¡ŒéªŒè¯
python verify_qwen_setup.py
```

## æ­¥éª¤ 9: ä¿®æ”¹è®­ç»ƒè„šæœ¬é…ç½®

```bash
# åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒé…ç½®
cat > training_config.py << 'EOF'
from transformers import TrainingArguments

def get_training_args():
    return TrainingArguments(
        output_dir="./output",
        per_device_train_batch_size=2,  # A100é€‚åˆçš„batch size
        gradient_accumulation_steps=8,
        learning_rate=5e-5,
        num_train_epochs=3,
        warmup_steps=100,
        logging_steps=10,
        save_steps=500,
        eval_steps=500,
        save_total_limit=3,
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        fp16=True,  # æ··åˆç²¾åº¦è®­ç»ƒ
        dataloader_num_workers=4,
        group_by_length=True,
        report_to="tensorboard",
        run_name="qwen2.5-vl-finetune",
    )

def get_attention_implementation():
    """æ ¹æ®ç¯å¢ƒè¿”å›æœ€ä½³çš„attentionå®ç°"""
    try:
        import flash_attn
        return "flash_attention_2"
    except ImportError:
        return "sdpa"  # PyTorchçš„scaled dot product attention
EOF
```

## æ­¥éª¤ 10: ä¿®æ”¹ä½ çš„è®­ç»ƒè„šæœ¬

åœ¨ä½ çš„ `train_qwen.py` ä¸­ï¼š

```python
# å¯¼å…¥é…ç½®
from training_config import get_training_args, get_attention_implementation

def train():
    # ä½¿ç”¨è‡ªé€‚åº”çš„attentionå®ç°
    attn_implementation = get_attention_implementation()
    print(f"ä½¿ç”¨attentionå®ç°: {attn_implementation}")
    
    # ä½¿ç”¨ä¼˜åŒ–çš„è®­ç»ƒå‚æ•°
    training_args = get_training_args()
    
    # åˆ›å»ºtrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        # ... å…¶ä»–å‚æ•°
    )
    
    # ä»å¤´å¼€å§‹è®­ç»ƒï¼Œä¸ä½¿ç”¨checkpoint
    trainer.train()  # ç¡®ä¿æ²¡æœ‰resume_from_checkpointå‚æ•°

if __name__ == "__main__":
    train()
```

## æ•…éšœæ’é™¤å¿«é€Ÿå‚è€ƒ

### å¦‚æœTransformerså¯¼å…¥å¤±è´¥:
```bash
pip uninstall transformers -y
pip install transformers==4.45.2
```

### å¦‚æœFlash Attentionç¼–è¯‘å¤±è´¥:
```bash
# è·³è¿‡Flash Attentionï¼Œä½¿ç”¨å…¶ä»–å®ç°
# åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨: attn_implementation="sdpa"
```

### å¦‚æœå†…å­˜ä¸è¶³:
```bash
# å‡å°batch size
per_device_train_batch_size=1
gradient_accumulation_steps=16
```

### å¦‚æœCUDAç‰ˆæœ¬ä¸åŒ¹é…:
```bash
# é‡æ–°å®‰è£…åŒ¹é…çš„PyTorchç‰ˆæœ¬
pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121
```

## ç¯å¢ƒä¿å­˜

```bash
# ä¿å­˜å·¥ä½œç¯å¢ƒ
conda env export > qwen-vl-final-environment.yml
pip freeze > requirements-final.txt

# æ—¥åæ¢å¤ç¯å¢ƒ
# conda env create -f qwen-vl-final-environment.yml
```

## æœ€ç»ˆæ£€æŸ¥æ¸…å•

- [ ] Python 3.11 ç¯å¢ƒåˆ›å»ºæˆåŠŸ
- [ ] CUDA_HOME æ­£ç¡®è®¾ç½®
- [ ] PyTorch CUDA åŠŸèƒ½æ­£å¸¸
- [ ] Transformers å’Œ Accelerate ç‰ˆæœ¬å…¼å®¹
- [ ] Qwen2.5-VL æ¨¡å‹å¯ä»¥å¯¼å…¥
- [ ] è®­ç»ƒè„šæœ¬å»é™¤checkpointæ¢å¤
- [ ] éªŒè¯è„šæœ¬è¿è¡ŒæˆåŠŸ
