# Qwen2.5-VL -- Linux A100 CUDA 12.7

## Step 1 -- Create New Environment

```bash
conda deactivate
conda env remove -n qwenvl

# clean cache.
pip cache purge
rm -rf ~/.cache/huggingface/
rm -rf ~/.cache/torch/
```

```bash
conda create -n qwenvl-new python=3.11 -y
conda activate qwenvl-new

conda update conda -y
pip install --upgrade pip setuptools wheel
```

## Step 2 -- Setup CUDA Environment

```bash
# æ£€æŸ¥å¯ç”¨çš„CUDAç‰ˆæœ¬
ls /usr/local/cuda*/bin/nvcc

# è®¾ç½®CUDA_HOME (æ ¹æ®ä½ çš„ç³»ç»Ÿé€‰æ‹©12.4æˆ–12.6)
# replace the path after export CUDA_HOME=
export CUDA_HOME=/usr/local/cuda-12.4
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# æ°¸ä¹…ä¿å­˜åˆ°bashrc
echo 'export CUDA_HOME=/usr/local/cuda-12.4' >> ~/.bashrc
echo 'export PATH=$CUDA_HOME/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# éªŒè¯CUDAè®¾ç½®
nvcc --version
nvidia-smi
```

## Step 3 -- Install PyTorch

```bash
# PyTorch 2.4.0 (CUDA 12.1+)
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121

# éªŒè¯PyTorchå®‰è£…
python -c "
import torch
print(f'PyTorch Version: {torch.__version__}')
print(f'CUDA Available: {torch.cuda.is_available()}')
print(f'CUDA Version: {torch.version.cuda}')
print(f'GPU Count: {torch.cuda.device_count()}')
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
"
```

## Step 4 -- Install Transformers

Here I refer to issue from official repo: https://github.com/QwenLM/Qwen2.5-VL/issues/936.

```bash
pip install accelerate==1.8.0.dev0
pip install transformers==4.49.0
pip install tokenizers==0.21.1
pip install datasets==3.2.0
pip install peft==0.13.2

python -c "
import transformers
import accelerate
print(f'Transformers: {transformers.__version__}')
print(f'Accelerate: {accelerate.__version__}')

try:
    from transformers import Trainer
    print('Trainer Import Success!')
except ImportError as e:
    print(f'Trainer Import Failed: {e}')
"
```

## Step 5 -- Install Flash-Attention

```bash
pip install packaging ninja wheel

# æ–¹æ³•1: å°è¯•å®‰è£…Flash Attention (å¯èƒ½éœ€è¦15minç¼–è¯‘æ—¶é—´)
pip install flash-attn==2.6.3 --no-build-isolation

# å¦‚æœç¼–è¯‘å¤±è´¥ï¼Œå°è¯•æ–¹æ³•2: ä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬
# pip install flash-attn --find-links https://github.com/Dao-AILab/flash-attention/releases

python -c "
try:
    import flash_attn
    print(f'Flash Attention Version: {flash_attn.__version__}')
except ImportError:
    print('Flash Attention Not Installed!')
"
```

```bash
# My Install Log: 
(qwenvl-new) $ pip show torch flash-attn deepspeed
Name: torch
Version: 2.4.0+cu121
Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Home-page: https://pytorch.org/
Author: PyTorch Team
Author-email: packages@pytorch.org
License: BSD-3
Location: /home/jack/anaconda3/envs/qwenvl-new/lib/python3.11/site-packages
Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions
Required-by: accelerate, bitsandbytes, deepspeed, flash-attn, peft, torchaudio, torchvision
---
Name: flash-attn
Version: 2.6.3
Summary: Flash Attention: Fast and Memory-Efficient Exact Attention
Home-page: https://github.com/Dao-AILab/flash-attention
Author: Tri Dao
Author-email: tri@tridao.me
License: 
Location: /home/jack/anaconda3/envs/qwenvl-new/lib/python3.11/site-packages
Requires: einops, torch
Required-by: 
---
Name: deepspeed
Version: 0.15.4
Summary: DeepSpeed library
Home-page: http://deepspeed.ai
Author: DeepSpeed Team
Author-email: deepspeed-info@microsoft.com
License: Apache Software License 2.0
Location: /home/jack/anaconda3/envs/qwenvl-new/lib/python3.11/site-packages
Requires: hjson, msgpack, ninja, numpy, nvidia-ml-py, packaging, psutil, py-cpuinfo, pydantic, torch, tqdm
Required-by: 
```

## Step 6 -- Install Other Dependencies

```bash
pip install deepspeed==0.15.4
pip install bitsandbytes==0.44.1

# å®‰è£…å›¾åƒå¤„ç†åº“
pip install Pillow==10.4.0
pip install opencv-python==4.10.0.84

# å®‰è£…å·¥å…·åº“
pip install tqdm==4.67.1
pip install tensorboard==2.19.0
pip install scikit-learn
pip install matplotlib seaborn
pip install wandb # æˆ‘åœ¨è¿™é‡Œæ²¡ç”¨ï¼Œoptional

# å®‰è£…Qwenç‰¹å®šä¾èµ–
pip install tiktoken==0.9.0
pip install transformers_stream_generator==0.0.5
```

## Step 7 -- Verification

``bash
python3 qwen-vl-finetune/finetune-env/test_env.py
```

```bash
# here is Chinese version.
cat > verify_qwen_setup.py << 'EOF'
#!/usr/bin/env python3
#!/usr/bin/env python3
import sys
import importlib
import traceback

def check_basic_environment():
    """æ£€æŸ¥åŸºç¡€ç¯å¢ƒ"""
    print("=== åŸºç¡€ç¯å¢ƒæ£€æŸ¥ ===")
    
    import torch
    import transformers
    
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")

def check_transformers_modules():
    """æ£€æŸ¥ transformers æ¨¡å—ç»“æ„"""
    print("\n=== Transformers æ¨¡å—ç»“æ„æ£€æŸ¥ ===")
    
    import transformers
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ models ç›®å½•
    try:
        import transformers.models
        print("âœ… transformers.models å­˜åœ¨")
        
        # åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹
        models_dir = transformers.models.__path__[0]
        import os
        available_models = [d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]
        
        print(f"å¯ç”¨æ¨¡å‹æ•°é‡: {len(available_models)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ qwen ç›¸å…³æ¨¡å‹
        qwen_models = [m for m in available_models if 'qwen' in m.lower()]
        print(f"Qwenç›¸å…³æ¨¡å‹: {qwen_models}")
        
    except Exception as e:
        print(f"âŒ transformers.models æ£€æŸ¥å¤±è´¥: {e}")

def check_qwen_models():
    """æ£€æŸ¥ Qwen æ¨¡å‹æ”¯æŒ"""
    print("\n=== Qwen æ¨¡å‹æ”¯æŒæ£€æŸ¥ ===")
    
    # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹å¼
    import_attempts = [
        ("ç›´æ¥å¯¼å…¥ qwen2_5_vl", "transformers.models.qwen2_5_vl.modeling_qwen2_5_vl", "Qwen2_5VLForConditionalGeneration"),
        ("å¯¼å…¥ qwen2_vl", "transformers.models.qwen2_vl.modeling_qwen2_vl", "Qwen2VLForConditionalGeneration"),
        ("AutoModel å¯¼å…¥", "transformers", "AutoModelForCausalLM"),
        ("AutoModel VL å¯¼å…¥", "transformers", "AutoModelForVision2Seq"),
    ]
    
    successful_imports = []
    
    for name, module_path, class_name in import_attempts:
        try:
            module = importlib.import_module(module_path)
            model_class = getattr(module, class_name)
            print(f"âœ… {name}: {class_name} å¯ç”¨")
            successful_imports.append((name, module_path, class_name))
        except ImportError as e:
            print(f"âŒ {name}: {e}")
        except AttributeError as e:
            print(f"âŒ {name}: æ¨¡å—å­˜åœ¨ä½†ç±»ä¸å­˜åœ¨ - {e}")
        except Exception as e:
            print(f"âŒ {name}: æœªçŸ¥é”™è¯¯ - {e}")
    
    return successful_imports

def check_model_loading(successful_imports):
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\n=== æ¨¡å‹åŠ è½½æµ‹è¯• ===")
    
    if not successful_imports:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç±»ï¼Œè·³è¿‡åŠ è½½æµ‹è¯•")
        return
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸçš„å¯¼å…¥è¿›è¡Œæµ‹è¯•
    name, module_path, class_name = successful_imports[0]
    print(f"ä½¿ç”¨ {name} è¿›è¡Œæµ‹è¯•...")
    
    try:
        module = importlib.import_module(module_path)
        model_class = getattr(module, class_name)
        
        # æµ‹è¯• from_pretrained æ˜¯å¦å¯ç”¨
        if hasattr(model_class, 'from_pretrained'):
            print("âœ… from_pretrained æ–¹æ³•å¯ç”¨")
        else:
            print("âŒ from_pretrained æ–¹æ³•ä¸å¯ç”¨")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹ç±»æµ‹è¯•å¤±è´¥: {e}")

def check_attention_implementations():
    """æ£€æŸ¥ attention å®ç°"""
    print("\n=== Attention å®ç°æ£€æŸ¥ ===")
    
    # æ£€æŸ¥ Flash Attention
    try:
        import flash_attn
        print(f"âœ… Flash Attention: {flash_attn.__version__}")
        
        # æ£€æŸ¥æ ¸å¿ƒå‡½æ•°
        try:
            from flash_attn import flash_attn_func
            print("âœ… flash_attn_func å¯ç”¨")
        except ImportError:
            print("âŒ flash_attn_func ä¸å¯ç”¨")
            
    except ImportError:
        print("âŒ Flash Attention æœªå®‰è£…")
    
    # æ£€æŸ¥ PyTorch SDPA
    try:
        import torch.nn.functional as F
        if hasattr(F, 'scaled_dot_product_attention'):
            print("âœ… PyTorch SDPA å¯ç”¨")
        else:
            print("âŒ PyTorch SDPA ä¸å¯ç”¨")
    except Exception as e:
        print(f"âŒ SDPA æ£€æŸ¥å¤±è´¥: {e}")

def suggest_solutions(successful_imports):
    """å»ºè®®è§£å†³æ–¹æ¡ˆ"""
    print("\n=== å»ºè®®è§£å†³æ–¹æ¡ˆ ===")
    
    if not successful_imports:
        print("ğŸ”§ ä¸»è¦é—®é¢˜ï¼šQwen2.5-VL æ¨¡å‹ç±»ä¸å¯ç”¨")
        print("\næ¨èè§£å†³æ–¹æ¡ˆï¼š")
        print("1. å‡çº§åˆ°æ”¯æŒ Qwen2.5-VL çš„ transformers ç‰ˆæœ¬ï¼š")
        print("   pip install transformers>=4.46.0")
        print("2. æˆ–è€…ä½¿ç”¨å¼€å‘ç‰ˆæœ¬ï¼š")
        print("   pip install git+https://github.com/huggingface/transformers.git")
        print("3. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„æ¨¡å‹åç§°")
        
    else:
        print("âœ… å‘ç°å¯ç”¨çš„æ¨¡å‹ç±»:")
        for name, module_path, class_name in successful_imports:
            print(f"   - {name}: {module_path}.{class_name}")
        
        print("\næ¨èä½¿ç”¨ç­–ç•¥ï¼š")
        print("1. ä¼˜å…ˆä½¿ç”¨ Qwen2_5VLForConditionalGeneration")
        print("2. å¦‚æœä¸å¯ç”¨ï¼Œä½¿ç”¨ AutoModelForCausalLM")
        print("3. è®¾ç½® trust_remote_code=True")

def generate_working_import_code(successful_imports):
    """ç”Ÿæˆå¯å·¥ä½œçš„å¯¼å…¥ä»£ç """
    print("\n=== ç”Ÿæˆå¯å·¥ä½œçš„å¯¼å…¥ä»£ç  ===")
    
    if successful_imports:
        name, module_path, class_name = successful_imports[0]
        
        print("åœ¨ä½ çš„ä»£ç ä¸­ä½¿ç”¨ä»¥ä¸‹å¯¼å…¥ï¼š")
        print("```python")
        print(f"# æ–¹æ³•1: ç›´æ¥å¯¼å…¥")
        print(f"from {module_path} import {class_name}")
        print()
        print("# æ–¹æ³•2: å®‰å…¨å¯¼å…¥")
        print("try:")
        print(f"    from {module_path} import {class_name}")
        print("except ImportError:")
        print("    from transformers import AutoModelForCausalLM as " + class_name)
        print("```")
    else:
        print("å»ºè®®ä½¿ç”¨é€šç”¨å¯¼å…¥ï¼š")
        print("```python")
        print("from transformers import AutoModelForCausalLM")
        print("from transformers import AutoTokenizer, AutoProcessor")
        print()
        print("# åŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨")
        print("model = AutoModelForCausalLM.from_pretrained(")
        print("    model_path,")
        print("    trust_remote_code=True,")
        print("    torch_dtype=torch.float16")
        print(")")
        print("```")

def main():
    """ä¸»è¯Šæ–­æµç¨‹"""
    print("ğŸ” å¼€å§‹ Qwen2.5-VL ç¯å¢ƒè¯Šæ–­...\n")
    
    try:
        # åŸºç¡€ç¯å¢ƒæ£€æŸ¥
        check_basic_environment()
        
        # æ¨¡å—ç»“æ„æ£€æŸ¥
        check_transformers_modules()
        
        # Qwen æ¨¡å‹æ£€æŸ¥
        successful_imports = check_qwen_models()
        
        # æ¨¡å‹åŠ è½½æµ‹è¯•
        check_model_loading(successful_imports)
        
        # Attention å®ç°æ£€æŸ¥
        check_attention_implementations()
        
        # å»ºè®®è§£å†³æ–¹æ¡ˆ
        suggest_solutions(successful_imports)
        
        # ç”Ÿæˆå·¥ä½œä»£ç 
        generate_working_import_code(successful_imports)
        
    except Exception as e:
        print(f"\nâŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
    
    print("\nğŸ¯ è¯Šæ–­å®Œæˆï¼")

if __name__ == "__main__":
    main()
    sys.exit(0 if success else 1)
EOF

# è¿è¡ŒéªŒè¯
python verify_qwen_setup.py
```


## Appendix -- Save Environment

```bash
conda env export > qwenvl-env.yml
pip freeze > requirements.txt

# æ¢å¤ç¯å¢ƒ
# conda env create -f qwen-vl-final-environment.yml
```

