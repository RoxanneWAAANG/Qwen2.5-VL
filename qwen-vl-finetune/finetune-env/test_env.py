#!/usr/bin/env python3
"""
Qwen2.5-VL é—®é¢˜è¯Šæ–­è„šæœ¬
å¸®åŠ©ç¡®å®šå…·ä½“çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
"""

import sys
import importlib
import traceback

def check_basic_environment():
    """æ£€æŸ¥åŸºç¡€ç¯å¢ƒ"""
    print("=== åŸºç¡€ç¯å¢ƒæ£€æŸ¥ ===")
    
    import torch
    import transformers
    
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")

def check_transformers_modules():
    """æ£€æŸ¥ transformers æ¨¡å—ç»“æ„"""
    print("\n=== Transformers æ¨¡å—ç»“æ„æ£€æŸ¥ ===")
    
    import transformers
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ models ç›®å½•
    try:
        import transformers.models
        print("âœ… transformers.models å­˜åœ¨")
        
        # åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹
        models_dir = transformers.models.__path__[0]
        import os
        available_models = [d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]
        
        print(f"å¯ç”¨æ¨¡å‹æ•°é‡: {len(available_models)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ qwen ç›¸å…³æ¨¡å‹
        qwen_models = [m for m in available_models if 'qwen' in m.lower()]
        print(f"Qwenç›¸å…³æ¨¡å‹: {qwen_models}")
        
    except Exception as e:
        print(f"âŒ transformers.models æ£€æŸ¥å¤±è´¥: {e}")

def check_qwen_models():
    """æ£€æŸ¥ Qwen æ¨¡å‹æ”¯æŒ"""
    print("\n=== Qwen æ¨¡å‹æ”¯æŒæ£€æŸ¥ ===")
    
    # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹å¼
    import_attempts = [
        ("ç›´æ¥å¯¼å…¥ qwen2_5_vl", "transformers.models.qwen2_5_vl.modeling_qwen2_5_vl", "Qwen2_5VLForConditionalGeneration"),
        ("å¯¼å…¥ qwen2_vl", "transformers.models.qwen2_vl.modeling_qwen2_vl", "Qwen2VLForConditionalGeneration"),
        ("AutoModel å¯¼å…¥", "transformers", "AutoModelForCausalLM"),
        ("AutoModel VL å¯¼å…¥", "transformers", "AutoModelForVision2Seq"),
    ]
    
    successful_imports = []
    
    for name, module_path, class_name in import_attempts:
        try:
            module = importlib.import_module(module_path)
            model_class = getattr(module, class_name)
            print(f"âœ… {name}: {class_name} å¯ç”¨")
            successful_imports.append((name, module_path, class_name))
        except ImportError as e:
            print(f"âŒ {name}: {e}")
        except AttributeError as e:
            print(f"âŒ {name}: æ¨¡å—å­˜åœ¨ä½†ç±»ä¸å­˜åœ¨ - {e}")
        except Exception as e:
            print(f"âŒ {name}: æœªçŸ¥é”™è¯¯ - {e}")
    
    return successful_imports

def check_model_loading(successful_imports):
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\n=== æ¨¡å‹åŠ è½½æµ‹è¯• ===")
    
    if not successful_imports:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç±»ï¼Œè·³è¿‡åŠ è½½æµ‹è¯•")
        return
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸçš„å¯¼å…¥è¿›è¡Œæµ‹è¯•
    name, module_path, class_name = successful_imports[0]
    print(f"ä½¿ç”¨ {name} è¿›è¡Œæµ‹è¯•...")
    
    try:
        module = importlib.import_module(module_path)
        model_class = getattr(module, class_name)
        
        # æµ‹è¯• from_pretrained æ˜¯å¦å¯ç”¨
        if hasattr(model_class, 'from_pretrained'):
            print("âœ… from_pretrained æ–¹æ³•å¯ç”¨")
        else:
            print("âŒ from_pretrained æ–¹æ³•ä¸å¯ç”¨")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹ç±»æµ‹è¯•å¤±è´¥: {e}")

def check_attention_implementations():
    """æ£€æŸ¥ attention å®ç°"""
    print("\n=== Attention å®ç°æ£€æŸ¥ ===")
    
    # æ£€æŸ¥ Flash Attention
    try:
        import flash_attn
        print(f"âœ… Flash Attention: {flash_attn.__version__}")
        
        # æ£€æŸ¥æ ¸å¿ƒå‡½æ•°
        try:
            from flash_attn import flash_attn_func
            print("âœ… flash_attn_func å¯ç”¨")
        except ImportError:
            print("âŒ flash_attn_func ä¸å¯ç”¨")
            
    except ImportError:
        print("âŒ Flash Attention æœªå®‰è£…")
    
    # æ£€æŸ¥ PyTorch SDPA
    try:
        import torch.nn.functional as F
        if hasattr(F, 'scaled_dot_product_attention'):
            print("âœ… PyTorch SDPA å¯ç”¨")
        else:
            print("âŒ PyTorch SDPA ä¸å¯ç”¨")
    except Exception as e:
        print(f"âŒ SDPA æ£€æŸ¥å¤±è´¥: {e}")

def suggest_solutions(successful_imports):
    """å»ºè®®è§£å†³æ–¹æ¡ˆ"""
    print("\n=== å»ºè®®è§£å†³æ–¹æ¡ˆ ===")
    
    if not successful_imports:
        print("ğŸ”§ ä¸»è¦é—®é¢˜ï¼šQwen2.5-VL æ¨¡å‹ç±»ä¸å¯ç”¨")
        print("\næ¨èè§£å†³æ–¹æ¡ˆï¼š")
        print("1. å‡çº§åˆ°æ”¯æŒ Qwen2.5-VL çš„ transformers ç‰ˆæœ¬ï¼š")
        print("   pip install transformers>=4.46.0")
        print("2. æˆ–è€…ä½¿ç”¨å¼€å‘ç‰ˆæœ¬ï¼š")
        print("   pip install git+https://github.com/huggingface/transformers.git")
        print("3. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„æ¨¡å‹åç§°")
        
    else:
        print("âœ… å‘ç°å¯ç”¨çš„æ¨¡å‹ç±»:")
        for name, module_path, class_name in successful_imports:
            print(f"   - {name}: {module_path}.{class_name}")
        
        print("\næ¨èä½¿ç”¨ç­–ç•¥ï¼š")
        print("1. ä¼˜å…ˆä½¿ç”¨ Qwen2_5VLForConditionalGeneration")
        print("2. å¦‚æœä¸å¯ç”¨ï¼Œä½¿ç”¨ AutoModelForCausalLM")
        print("3. è®¾ç½® trust_remote_code=True")

def generate_working_import_code(successful_imports):
    """ç”Ÿæˆå¯å·¥ä½œçš„å¯¼å…¥ä»£ç """
    print("\n=== ç”Ÿæˆå¯å·¥ä½œçš„å¯¼å…¥ä»£ç  ===")
    
    if successful_imports:
        name, module_path, class_name = successful_imports[0]
        
        print("åœ¨ä½ çš„ä»£ç ä¸­ä½¿ç”¨ä»¥ä¸‹å¯¼å…¥ï¼š")
        print("```python")
        print(f"# æ–¹æ³•1: ç›´æ¥å¯¼å…¥")
        print(f"from {module_path} import {class_name}")
        print()
        print("# æ–¹æ³•2: å®‰å…¨å¯¼å…¥")
        print("try:")
        print(f"    from {module_path} import {class_name}")
        print("except ImportError:")
        print("    from transformers import AutoModelForCausalLM as " + class_name)
        print("```")
    else:
        print("å»ºè®®ä½¿ç”¨é€šç”¨å¯¼å…¥ï¼š")
        print("```python")
        print("from transformers import AutoModelForCausalLM")
        print("from transformers import AutoTokenizer, AutoProcessor")
        print()
        print("# åŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨")
        print("model = AutoModelForCausalLM.from_pretrained(")
        print("    model_path,")
        print("    trust_remote_code=True,")
        print("    torch_dtype=torch.float16")
        print(")")
        print("```")

def main():
    """ä¸»è¯Šæ–­æµç¨‹"""
    print("ğŸ” å¼€å§‹ Qwen2.5-VL ç¯å¢ƒè¯Šæ–­...\n")
    
    try:
        # åŸºç¡€ç¯å¢ƒæ£€æŸ¥
        check_basic_environment()
        
        # æ¨¡å—ç»“æ„æ£€æŸ¥
        check_transformers_modules()
        
        # Qwen æ¨¡å‹æ£€æŸ¥
        successful_imports = check_qwen_models()
        
        # æ¨¡å‹åŠ è½½æµ‹è¯•
        check_model_loading(successful_imports)
        
        # Attention å®ç°æ£€æŸ¥
        check_attention_implementations()
        
        # å»ºè®®è§£å†³æ–¹æ¡ˆ
        suggest_solutions(successful_imports)
        
        # ç”Ÿæˆå·¥ä½œä»£ç 
        generate_working_import_code(successful_imports)
        
    except Exception as e:
        print(f"\nâŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
    
    print("\nğŸ¯ è¯Šæ–­å®Œæˆï¼")

if __name__ == "__main__":
    main()